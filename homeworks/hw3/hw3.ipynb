{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1. \n",
    "===\n",
    "Create a computational graph for the following expression:\n",
    "$$(x*y + z+1/x)* w = f$$\n",
    "Calculate the forward values of all the nodes and function f starting with $x = -1, y = 2, z = 4, w = 5$. Subsequently, determine backward values, and finally the derivatives of  f with respect with x, y, z and w. Please, present your results as a simple graph. You can draw you graph by any means you find convenient, including by hand. Please place forward values above the lines representing propagation of values and backpropagation values (derivatives) below the lines.\n",
    "(25%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "---\n",
    "* Black numbers are calculations for f starting with the given x, y, z, w values\n",
    "* Red numbers are back-propagated derivative values\n",
    "* NOTE: as there are two calculation paths from x, we need to add up the two back propagated derivative values to x\n",
    "![hw3_p1.png](hw3_p1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2. \n",
    "===\n",
    "Create a computational graph for the following expression:\n",
    "$$f =\\frac{x+ \\sigma(y)}{\\sigma(z)+(x+y)^2}$$\n",
    "Where$$\\sigma(q) = \\frac{1}{1+ e^{-q}}$$\n",
    "Calculate forward computational values of all nodes and the derivatives of function f with respect to x,y and z. Please, present your results as a simple graph. Please place forward values above the lines representing propagation of values and backpropagation values (derivatives) below the lines. Perform your calculations manually. \n",
    "As values for x,y,z use (2, -4, 3)\n",
    "(25%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "---\n",
    "* Representing Sigmoid function as a single node\n",
    "* Black numbers are calculations for f starting with given x, y, z values\n",
    "* Red numbers are back-propagated derivative values with precision of 4\n",
    "![hw3_p2.png](hw3_p2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3. \n",
    "===\n",
    "Please perform calculations in problem 2 using TensorFlow. As you are moving forward, please calculate and store (cash) values of various derivatives you will need for backward calculations. \n",
    "(25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Calculated  f  =  0.40746209025382996\n",
      "Back propagation   dx  =  0.5310063362121582\n",
      "Back propagation   dy  =  0.33265751600265503\n",
      "Back propagation   dz  =  -0.003716809442266822\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def sigma(x):\n",
    "    return tf.div(tf.constant(1.0),\n",
    "                  tf.add(tf.constant(1.0), tf.exp(tf.negative(x))))\n",
    "\n",
    "def sigmaprime(x):\n",
    "    return tf.multiply(sigma(x), tf.subtract(tf.constant(1.0), sigma(x)))\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "x_sigY = x + sigma(y)\n",
    "\n",
    "x_y_2 = (x + y)**2\n",
    "\n",
    "sigZ_x_y_2 = sigma(z) + x_y_2\n",
    "\n",
    "f = tf.div(x_sigY, sigZ_x_y_2)\n",
    "\n",
    "\n",
    "# Back propagation\n",
    "\n",
    "# Upper branch of x\n",
    "d_x_1 = tf.div(tf.constant(1, dtype=tf.float32), sigZ_x_y_2)\n",
    "\n",
    "# Upper branch of y\n",
    "d_y_1 = tf.multiply(sigmaprime(y), d_x_1)\n",
    "\n",
    "d_y_z = tf.div(tf.negative(x_sigY), sigZ_x_y_2**2)\n",
    "d_x_y_2 = 2*(x+y)\n",
    "\n",
    "d_y_2 = d_x_y_2 * d_y_z\n",
    "d_x_2 = d_y_2\n",
    "\n",
    "d_x = d_x_1 + d_x_2\n",
    "d_y = d_y_1 + d_y_2\n",
    "d_z = tf.multiply(sigmaprime(z), d_y_z)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run([f, d_x, d_y, d_z], feed_dict={x: 2, y: -4, z: 3})\n",
    "    print(\"Forward Calculated  f  =  {}\".format(result[0]))\n",
    "    print(\"Back propagation   dx  =  {}\".format(result[1]))\n",
    "    print(\"Back propagation   dy  =  {}\".format(result[2]))\n",
    "    print(\"Back propagation   dz  =  {}\".format(result[3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4. \n",
    "===\n",
    "Please examine attached Python code for classification of MNIST hand written digits dataset. Please try to find “optimal” values of two hyper parameters: learning_rate and batch_size. Optimal is a vague term. You would like to achieve the best accuracy in the shortest possible time. Please do not sweat it out. \n",
    "(25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-1-3ddb6aacd3dd>:44: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Average loss epoch 0: 0.6200318447227686\n",
      "Average loss epoch 1: 0.39352775723393496\n",
      "Average loss epoch 2: 0.35750633966727335\n",
      "Average loss epoch 3: 0.3388547466996589\n",
      "Average loss epoch 4: 0.32706829347734245\n",
      "Average loss epoch 5: 0.31817121600192755\n",
      "Average loss epoch 6: 0.31099098273476616\n",
      "Average loss epoch 7: 0.30674878495638486\n",
      "Average loss epoch 8: 0.30279367152458986\n",
      "Average loss epoch 9: 0.2980282822104751\n",
      "Average loss epoch 10: 0.29580685146193686\n",
      "Average loss epoch 11: 0.29321130267420753\n",
      "Average loss epoch 12: 0.28969128307749015\n",
      "Average loss epoch 13: 0.2879804641904075\n",
      "Average loss epoch 14: 0.28545451494025403\n",
      "Average loss epoch 15: 0.2846681862010982\n",
      "Average loss epoch 16: 0.282892616028017\n",
      "Average loss epoch 17: 0.2808123849780182\n",
      "Average loss epoch 18: 0.2806691644621677\n",
      "Average loss epoch 19: 0.2780856388351305\n",
      "Average loss epoch 20: 0.2772121587819089\n",
      "Average loss epoch 21: 0.27655668275766687\n",
      "Average loss epoch 22: 0.27607008342534467\n",
      "Average loss epoch 23: 0.27447662602275447\n",
      "Average loss epoch 24: 0.27264406988040996\n",
      "Average loss epoch 25: 0.2720479331503475\n",
      "Average loss epoch 26: 0.2720791964583058\n",
      "Average loss epoch 27: 0.2714691780955414\n",
      "Average loss epoch 28: 0.26925906807672784\n",
      "Average loss epoch 29: 0.27000288832220226\n",
      "Total time: 12.000133275985718 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.915\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple logistic regression model to solve OCR task \n",
    "with MNIST in TensorFlow\n",
    "MNIST dataset: yann.lecun.com/exdb/mnist/\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.1\n",
    "batch_size = 150\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder mnist\n",
    "mnist = input_data.read_data_sets('./mnist', one_hot=True) \n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\t\n",
    "\t\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\n",
    "\twriter.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "---\n",
    "Learning Rate: 0.1\n",
    "\n",
    "Batch Size : 150\n",
    "\n",
    "Total Time: 12.00\n",
    "\n",
    "Accuracy : 0.915"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
